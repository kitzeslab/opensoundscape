{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with Opensoundscape & Pytorch Lightning\n",
    "\n",
    "OpenSoundscape provides classes that support the use of Pytorch Lightning's Trainer class, which implements various training techniques, speedups, and utilities. To use Lightning, simply use the `opensoundscape.ml.lightning.LightningSpectrogramModule` class rather than the `opensoundscape.ml.cnn.SpectrogramClassifier` class (or `CNN` class, which is now an alias for `SpectrogramClassifier`). For the most part, the API and functionality is similar to the pure-pytorch classes, with a few major differences:\n",
    "- to train, call the `.fit_with_trainer()` method (\"train()\" method is reserved for other purposes when using Lightning). Pass any kwargs to lightning.Trainer()to customize the Lightning Trainer. \n",
    "- to predict, call `.predict_with_trainer()`, passing any kwargs for the lightning.Trainer init with `lightning_trainer_kwargs=dict(...)`\n",
    "- note that with the Lightning Trainer, you can use various logging platforms, while only Weights and Biases is currently supported in the pure PyTorch classes\n",
    "\n",
    "Check out the lightning.Trainer [docs](https://lightning.ai/docs/pytorch/stable/common/trainer.html) for the full set of implemented features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this is a Google Colab notebook, install opensoundscape in the runtime environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  %pip install git+https://github.com/kitzeslab/opensoundscape@develop ipykernel==5.5.6 ipython==7.34.0 pillow==9.4.0\n",
    "  num_workers=0\n",
    "else:\n",
    "  num_workers=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the cnn module provides classes for training/predicting with various types of CNNs\n",
    "from opensoundscape import CNN\n",
    "\n",
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import subprocess\n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds\n",
    "\n",
    "Set manual seeds for Pytorch and Python. These essentially \"fix\" the results of any stochastic steps in model training, ensuring that training results are reproducible. You probably don't want to do this when you actually train your model, but it's useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download files\n",
    "\n",
    "Training a machine learning model requires some pre-labeled data. These data, in the form of audio recordings or spectrograms, are labeled with whether or not they contain the sound of the species of interest. \n",
    "\n",
    "These data can be obtained from online databases such as Xeno-Canto.org, or by labeling one's own ARU data using a program like Cornell's Raven sound analysis software. In this example we are using a set of annotated avian soundscape recordings that were annotated using the software Raven Pro 1.6.4 (Bioacoustics Research Program 2022):\n",
    "\n",
    "<blockquote><i>An annotated set of audio recordings of Eastern North American birds containing frequency, time, and species information. </i> Lauren M. Chronister,  Tessa A. Rhinehart,  Aidan Place,  Justin Kitzes.\n",
    "https://doi.org/10.1002/ecy.3329 \n",
    "</blockquote>\n",
    "\n",
    "These are the same data that are used by the annotation and preprocessing tutorials, so you can skip this step if you've already downloaded them there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download example files\n",
    "Download a set of example audio files and Raven annotations:\n",
    "\n",
    "Option 1: run the cell below\n",
    "\n",
    "- if you get a 403 error, DataDryad suspects you are a bot. Use Option 2. \n",
    "\n",
    "Option 2:\n",
    "\n",
    "- Download and unzip both `annotation_Files.zip` and `mp3_Files.zip` from the https://datadryad.org/stash/dataset/doi:10.5061/dryad.d2547d81z  \n",
    "- Move the unzipped contents into a subfolder of the current folder called `./annotated_data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-28 12:30:10--  https://datadryad.org/stash/downloads/file_stream/641805\n",
      "Resolving datadryad.org (datadryad.org)... 34.218.192.228, 52.27.226.201, 34.223.182.183, ...\n",
      "Connecting to datadryad.org (datadryad.org)|34.218.192.228|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2024-08-28 12:30:10 ERROR 403: Forbidden.\n",
      "\n",
      "--2024-08-28 12:30:11--  https://datadryad.org/stash/downloads/file_stream/641807\n",
      "Resolving datadryad.org (datadryad.org)... 52.27.226.201, 34.223.182.183, 35.80.241.54, ...\n",
      "Connecting to datadryad.org (datadryad.org)|52.27.226.201|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2024-08-28 12:30:11 ERROR 403: Forbidden.\n",
      "\n",
      "mkdir: annotated_data: File exists\n",
      "Archive:  annotation_Files.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of annotation_Files.zip or\n",
      "        annotation_Files.zip.zip, and cannot find annotation_Files.zip.ZIP, period.\n",
      "Archive:  mp3_Files.zip\n",
      "  End-of-central-directory signature not found.  Either this file is not\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
      "  latter case the central directory and zipfile comment will be found on\n",
      "  the last disk(s) of this archive.\n",
      "unzip:  cannot find zipfile directory in one of mp3_Files.zip or\n",
      "        mp3_Files.zip.zip, and cannot find mp3_Files.zip.ZIP, period.\n"
     ]
    }
   ],
   "source": [
    "# Note: the \"!\" preceding each line below allows us to run bash commands in a Jupyter notebook\n",
    "# If you are not running this code in a notebook, input these commands into your terminal instead\n",
    "!wget -O annotation_Files.zip https://datadryad.org/stash/downloads/file_stream/641805;\n",
    "!wget -O mp3_Files.zip https://datadryad.org/stash/downloads/file_stream/641807;\n",
    "!mkdir annotated_data;\n",
    "!unzip annotation_Files.zip -d ./annotated_data/annotation_Files;\n",
    "!unzip mp3_Files.zip -d ./annotated_data/mp3_Files;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and validation data\n",
    "\n",
    "To prepare audio data for machine learning, we need to convert our annotated data into clip-level labels.\n",
    "\n",
    "These steps are covered in depth in other tutorials, so we'll just set our clip labels up quickly for this example.\n",
    "\n",
    "First, get exactly matched lists of audio files and their corresponding selection files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/annotations.py:216: UserWarning: annotated_data/annotation_Files/Recording_1/Recording_1_Segment_01.Table.1.selections.txt has zero rows.\n",
      "  warnings.warn(f\"{raven_file} has zero rows.\")\n"
     ]
    }
   ],
   "source": [
    "# Set the current directory to where the dataset is downloaded\n",
    "dataset_path = Path(\"./annotated_data/\")\n",
    "\n",
    "# Make a list of all of the selection table files\n",
    "selection_files = glob(f\"{dataset_path}/annotation_Files/*/*.txt\")\n",
    "\n",
    "# Create a list of audio files, one corresponding to each Raven file\n",
    "# (Audio files have the same names as selection files with a different extension)\n",
    "audio_files = [f.replace('annotation_Files','mp3_Files').replace('.Table.1.selections.txt','.mp3') for f in selection_files]\n",
    "\n",
    "#Next, convert the selection files and audio files to a `BoxedAnnotations` object, which contains\n",
    "#the time, frequency, and label information for all annotations for every recording in the dataset.\n",
    "\n",
    "from opensoundscape.annotations import BoxedAnnotations\n",
    "# Create a dataframe of annotations\n",
    "annotations = BoxedAnnotations.from_raven_files(\n",
    "    selection_files,\n",
    "    audio_files)\n",
    "\n",
    "# Parameters to use for label creation\n",
    "clip_duration = 3\n",
    "clip_overlap = 0\n",
    "min_label_overlap = 0.25\n",
    "species_of_interest = [\"NOCA\", \"EATO\", \"SCTA\", \"BAWW\", \"BCCH\", \"AMCR\", \"NOFL\"]\n",
    "\n",
    "# Create dataframe of one-hot labels\n",
    "clip_labels = annotations.clip_labels(\n",
    "    clip_duration = clip_duration, \n",
    "    clip_overlap = clip_overlap,\n",
    "    min_label_overlap = min_label_overlap,\n",
    "    class_subset = species_of_interest # You can comment this line out if you want to include all species.\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(clip_labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lightning-copmatible model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a LightningSpectrogramModule object, which integrates OpenSoundscape with Pytorch Lightning's powerful Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN object designed to recognize 3-second samples\n",
    "from opensoundscape.ml.lightning import LightningSpectrogramModule\n",
    "\n",
    "# initializing it looks the same as for the CNN class.\n",
    "# Let's use resnet34 architecture and 3s clip duration\n",
    "model = LightningSpectrogramModule(\n",
    "    architecture = 'resnet34',\n",
    "    classes = clip_labels.columns.tolist(),\n",
    "    sample_duration = 3 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Lightning\n",
    "\n",
    "Lightning will take a bit of time to get things set up. After that, it can be substantially faster than training in pure PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "Missing logger folder: ./lightning_logs\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/SML161/opensoundscape/docs/tutorials exists and is not empty.\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/lightning/pytorch/core/optimizer.py:376: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "\n",
      "  | Name    | Type                  | Params\n",
      "--------------------------------------------------\n",
      "0 | network | ResNet                | 21.3 M\n",
      "1 | loss_fn | BCEWithLogitsLoss_hot | 0     \n",
      "--------------------------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.128    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7382303878214d5695a5095186977f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/torchmetrics/functional/classification/precision_recall_curve.py:798: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:334.)\n",
      "  unique_mapping = unique_mapping[unique_mapping >= 0]\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/torchmetrics/functional/classification/average_precision.py:308: UserWarning: MPS: no support for int64 for sum_out_mps, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:157.)\n",
      "  weights=(state[1] == 1).sum(dim=0).float() if thresholds is None else state[0][:, 1, :].sum(-1),\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6d760f79f04823a886348f6dd9e7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676afca0ec5a47149b1ddbe94cad5134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f92835195e4429999cbc3a8ff21c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a96160985f5464590523f8861844b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e65e4d664740408906c03a273b6b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete\n",
      "Best model with score 0.140 is saved to /Users/SML161/opensoundscape/docs/tutorials/epoch=1-step=194.ckpt\n",
      "0 of 6160 total training samples failed to preprocess\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightning.pytorch.trainer.trainer.Trainer at 0x2d191a610>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again, the API is very similar to CNN\n",
    "# but now, we can pass any kwargs to Lightning.Trainer() as well. For example, \n",
    "# let's use the `accum_grad_batches` argument to accumulate gradients over 2 batches before running the optimizer,\n",
    "# effectively doubling the batch size.\n",
    "model.fit_with_trainer(train_df, val_df, epochs=4, batch_size=32, num_workers=num_workers, accumulate_grad_batches=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:436: Consider setting `persistent_workers=True` in 'predict_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ac591893954f2a9475fc8c9acb67a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>NOCA</th>\n",
       "      <th>EATO</th>\n",
       "      <th>SCTA</th>\n",
       "      <th>BAWW</th>\n",
       "      <th>BCCH</th>\n",
       "      <th>AMCR</th>\n",
       "      <th>NOFL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_1/Recording_1_Segment_13.mp3</th>\n",
       "      <th>84.0</th>\n",
       "      <th>87.0</th>\n",
       "      <td>11.701532</td>\n",
       "      <td>10.014467</td>\n",
       "      <td>9.671937</td>\n",
       "      <td>1.633159</td>\n",
       "      <td>9.227418</td>\n",
       "      <td>8.859664</td>\n",
       "      <td>-0.052373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_4/Recording_4_Segment_13.mp3</th>\n",
       "      <th>87.0</th>\n",
       "      <th>90.0</th>\n",
       "      <td>9.945120</td>\n",
       "      <td>8.878679</td>\n",
       "      <td>9.042792</td>\n",
       "      <td>1.747799</td>\n",
       "      <td>8.751578</td>\n",
       "      <td>8.416622</td>\n",
       "      <td>0.018097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_1/Recording_1_Segment_07.mp3</th>\n",
       "      <th>249.0</th>\n",
       "      <th>252.0</th>\n",
       "      <td>9.504481</td>\n",
       "      <td>8.450200</td>\n",
       "      <td>8.734877</td>\n",
       "      <td>1.783901</td>\n",
       "      <td>8.292206</td>\n",
       "      <td>8.354047</td>\n",
       "      <td>0.422760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_4/Recording_4_Segment_11.mp3</th>\n",
       "      <th>231.0</th>\n",
       "      <th>234.0</th>\n",
       "      <td>9.832380</td>\n",
       "      <td>9.031016</td>\n",
       "      <td>9.131749</td>\n",
       "      <td>1.859293</td>\n",
       "      <td>9.226156</td>\n",
       "      <td>8.416931</td>\n",
       "      <td>0.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_1/Recording_1_Segment_35.mp3</th>\n",
       "      <th>228.0</th>\n",
       "      <th>231.0</th>\n",
       "      <td>10.225933</td>\n",
       "      <td>8.867890</td>\n",
       "      <td>9.046994</td>\n",
       "      <td>1.748763</td>\n",
       "      <td>8.689975</td>\n",
       "      <td>8.404178</td>\n",
       "      <td>0.139670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_1/Recording_1_Segment_25.mp3</th>\n",
       "      <th>186.0</th>\n",
       "      <th>189.0</th>\n",
       "      <td>10.359131</td>\n",
       "      <td>9.550800</td>\n",
       "      <td>9.299773</td>\n",
       "      <td>2.005651</td>\n",
       "      <td>9.608356</td>\n",
       "      <td>8.805997</td>\n",
       "      <td>0.035945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_1/Recording_1_Segment_27.mp3</th>\n",
       "      <th>138.0</th>\n",
       "      <th>141.0</th>\n",
       "      <td>10.066076</td>\n",
       "      <td>9.293936</td>\n",
       "      <td>9.277360</td>\n",
       "      <td>1.741633</td>\n",
       "      <td>8.826027</td>\n",
       "      <td>8.666476</td>\n",
       "      <td>-0.180602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_2/Recording_2_Segment_14.mp3</th>\n",
       "      <th>90.0</th>\n",
       "      <th>93.0</th>\n",
       "      <td>9.545153</td>\n",
       "      <td>8.941689</td>\n",
       "      <td>8.957946</td>\n",
       "      <td>1.810180</td>\n",
       "      <td>8.933350</td>\n",
       "      <td>8.355339</td>\n",
       "      <td>0.146811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_2/Recording_2_Segment_13.mp3</th>\n",
       "      <th>117.0</th>\n",
       "      <th>120.0</th>\n",
       "      <td>11.184624</td>\n",
       "      <td>9.599746</td>\n",
       "      <td>9.165170</td>\n",
       "      <td>1.693613</td>\n",
       "      <td>9.404591</td>\n",
       "      <td>8.947598</td>\n",
       "      <td>0.386571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annotated_data/mp3_Files/Recording_4/Recording_4_Segment_05.mp3</th>\n",
       "      <th>159.0</th>\n",
       "      <th>162.0</th>\n",
       "      <td>10.500993</td>\n",
       "      <td>9.556500</td>\n",
       "      <td>9.760592</td>\n",
       "      <td>1.945536</td>\n",
       "      <td>9.900855</td>\n",
       "      <td>9.640491</td>\n",
       "      <td>0.502536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1540 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             NOCA  \\\n",
       "file                                               start_time end_time              \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0      11.701532   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0       9.945120   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0      9.504481   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0      9.832380   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0     10.225933   \n",
       "...                                                                           ...   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0     10.359131   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0     10.066076   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0       9.545153   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0     11.184624   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0     10.500993   \n",
       "\n",
       "                                                                             EATO  \\\n",
       "file                                               start_time end_time              \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0      10.014467   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0       8.878679   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0      8.450200   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0      9.031016   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0      8.867890   \n",
       "...                                                                           ...   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0      9.550800   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0      9.293936   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0       8.941689   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0      9.599746   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0      9.556500   \n",
       "\n",
       "                                                                            SCTA  \\\n",
       "file                                               start_time end_time             \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0      9.671937   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0      9.042792   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0     8.734877   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0     9.131749   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0     9.046994   \n",
       "...                                                                          ...   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0     9.299773   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0     9.277360   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0      8.957946   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0     9.165170   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0     9.760592   \n",
       "\n",
       "                                                                            BAWW  \\\n",
       "file                                               start_time end_time             \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0      1.633159   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0      1.747799   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0     1.783901   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0     1.859293   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0     1.748763   \n",
       "...                                                                          ...   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0     2.005651   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0     1.741633   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0      1.810180   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0     1.693613   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0     1.945536   \n",
       "\n",
       "                                                                            BCCH  \\\n",
       "file                                               start_time end_time             \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0      9.227418   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0      8.751578   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0     8.292206   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0     9.226156   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0     8.689975   \n",
       "...                                                                          ...   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0     9.608356   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0     8.826027   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0      8.933350   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0     9.404591   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0     9.900855   \n",
       "\n",
       "                                                                            AMCR  \\\n",
       "file                                               start_time end_time             \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0      8.859664   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0      8.416622   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0     8.354047   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0     8.416931   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0     8.404178   \n",
       "...                                                                          ...   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0     8.805997   \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0     8.666476   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0      8.355339   \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0     8.947598   \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0     9.640491   \n",
       "\n",
       "                                                                            NOFL  \n",
       "file                                               start_time end_time            \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 84.0       87.0     -0.052373  \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 87.0       90.0      0.018097  \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 249.0      252.0     0.422760  \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 231.0      234.0     0.288400  \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 228.0      231.0     0.139670  \n",
       "...                                                                          ...  \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 186.0      189.0     0.035945  \n",
       "annotated_data/mp3_Files/Recording_1/Recording_... 138.0      141.0    -0.180602  \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 90.0       93.0      0.146811  \n",
       "annotated_data/mp3_Files/Recording_2/Recording_... 117.0      120.0     0.386571  \n",
       "annotated_data/mp3_Files/Recording_4/Recording_... 159.0      162.0     0.502536  \n",
       "\n",
       "[1540 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_with_trainer(val_df, batch_size=32, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps: \n",
    "experiment with the various optimizations and features of lightning.Trainer, such as integration with several different logging platforms, multi-device distributed training, and more. \n",
    "\n",
    "Check out the Lightning Trainer [docs](https://lightning.ai/docs/pytorch/stable/common/trainer.html) to learn more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# uncomment to remove the training files\n",
    "# shutil.rmtree('./annotated_data')\n",
    "\n",
    "shutil.rmtree('./wandb', ignore_errors=True)\n",
    "shutil.rmtree('./model_training_checkpoints', ignore_errors=True)\n",
    "for f in glob('./*.ckpt'):\n",
    "    Path(f).unlink()\n",
    "\n",
    "try:\n",
    "    Path('annotation_Files.zip').unlink()\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    Path('mp3_Files.zip').unlink()\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso_dev",
   "language": "python",
   "name": "opso_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
