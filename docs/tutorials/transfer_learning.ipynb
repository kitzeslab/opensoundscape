{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning: training shallow classifiers on embedding models outputs\n",
    "\n",
    "If you want to adapt BirdNET, Perch, HawkEars, or another foundation model to a new set of classes or a new domain, you'll be doing what machine learning experts call transfer learning. This tutorial demonstrates tranfer learning for PyToch models, but you can do the same with TensorFlow models such as BirdNET and Perch - examples are in a separate notebook `training_birdnet_and_perch.ipynb` since you might need to set your python environment up differently (by installing tensorflow and tensorflow-hub packages).\n",
    "\n",
    "This notebook shows examples of how to train simple one-layer or multi-layer fully-connected neural networks (aka multi-layer perceptron networks, MLPs) on embedding (aka features) generated by a pre-trained deep learning model. This workflow is called transfer learning because the learned feature extraction of the embedding model is transfered to a new domain. Ghani et al. [1] demonstrated that gobal bird classification models can act as feature extractors that can be used to train shallow classifiers on novel tasks and domains, even when few training samples are available.\n",
    "\n",
    "Training a shallow classifier on embeddings, rather than training or fine-tuning an entire deep learning model, has three advantages: (1) classifiers can be developed with just a handful of training examples; (2) models fit very quickly, enabling an iterative human-in-the-loop workflow for active learning; (3) any model that generates embeddings can be used as the feature extractor; in particular, compiled models without open-source weights (e.g. BirdNET [2]) can be used as the embedding model.\n",
    "\n",
    "Users can develop flexible and customizable transfer-learning workflow by generating embeddings then using PyTorch or sklearn directly. This notebook demonstrates both (1) high-level functions and classes in OpenSoundscape that simplify the code needed to perform transfer learning; and (2) examples demonstrating the embedding and model fitting steps explicitly line-by-line.\n",
    "\n",
    "> Note: to use models from the model zoo, install the bioacoustics_model_zoo as a package in your python environment:\n",
    "\n",
    "`pip install git+https://github.com/kitzeslab/bioacoustics-model-zoo`\n",
    "\n",
    "[1] Ghani, B., T. Denton, S. Kahl, H. Klinck, T. Denton, S. Kahl, and H. Klinck. 2023. Global birdsong embeddings enable superior transfer learning for bioacoustic classification. Scientific Reports 13:22876.\n",
    "\n",
    "[2] Kahl, Stefan, et al. \"BirdNET: A deep learning solution for avian diversity monitoring.\" Ecological Informatics 61 (2021): 101236.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this tutorial\n",
    "\n",
    "This tutorial is more than a reference! It's a Jupyter Notebook which you can run and modify on Google Colab or your own computer.\n",
    "\n",
    "|Link to tutorial|How to run tutorial|\n",
    "| :- | :- |\n",
    "| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kitzeslab/opensoundscape/blob/master/docs/tutorials/train_cnn.ipynb) | The link opens the tutorial in Google Colab. Uncomment the \"installation\" line in the first cell to install OpenSoundscape. |\n",
    "| [![Download via DownGit](https://img.shields.io/badge/GitHub-Download-teal?logo=github)](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/kitzeslab/opensoundscape/blob/master/docs/tutorials/train_cnn.ipynb) | The link downloads the tutorial file to your computer. Follow the [Jupyter installation instructions](https://opensoundscape.org/en/latest/installation/jupyter.html), then open the tutorial file in Jupyter. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this is a Google Colab notebook, install opensoundscape in the runtime environment\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  %pip install git+https://github.com/kitzeslab/opensoundscape@develop ipykernel==5.5.6 ipython==7.34.0 pillow==9.4.0\n",
    "  num_workers=0\n",
    "else:\n",
    "  #can use >0, e.g. 4, but might need to install the bioacoustics model zoo as a package:\n",
    "  # `pip install git+https://github.com/kitzeslab/bioacoustics-model-zoo@0.11.0.dev1`\n",
    "  num_workers=0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/265wdp1n0bn_r85dh3pp95fh0000gq/T/ipykernel_76442/267712919.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "#other utilities and packages\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from glob import glob\n",
    "import sklearn\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from pathlib import Path\n",
    "\n",
    "#set up plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize']=[15,5] #for large visuals\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# opensoundscape transfer learning tools\n",
    "from opensoundscape.ml.shallow_classifier import MLPClassifier, quick_fit, fit_classifier_on_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seeds\n",
    "\n",
    "Set manual seeds for Pytorch and Python. These essentially \"fix\" the results of any stochastic steps in model training, ensuring that training results are reproducible. You probably don't want to do this when you actually train your model, but it's useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download example files\n",
    "Download a set of aquatic soundscape recordings with annotations of _Rana sierrae_ vocalizations\n",
    "\n",
    "Option 1: run the cell below\n",
    "\n",
    "- if you get a 403 error, DataDryad suspects you are a bot. Use Option 2. \n",
    "\n",
    "Option 2:\n",
    "\n",
    "- Download and unzip the `rana_sierrae_2022.zip` folder containing audio and annotations from this [public Dryad dataset](https://datadryad.org/stash/dataset/doi:10.5061/dryad.9s4mw6mn3#readme)\n",
    "- Move the unzipped `rana_sierrae_2022` folder into the current folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: the \"!\" preceding each line below allows us to run bash commands in a Jupyter notebook\n",
    "# # If you are not running this code in a notebook, input these commands into your terminal instead\n",
    "# !wget -O rana_sierrae_2022.zip https://datadryad.org/stash/downloads/file_stream/2722802;\n",
    "# !unzip rana_sierrae_2022;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare audio data\n",
    "See the train_cnn.ipynb tutorial for step-by-step walkthrough of this process, or just run the cells below to prepare a trainig set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/annotations.py:300: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_annotations_df = pd.concat(all_file_dfs).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Set the current directory to where the folder `rana_sierrae_2022` is located:\n",
    "dataset_path = Path(\"./rana_sierrae_2022/\")\n",
    "\n",
    "# let's generate clip labels of 5s duration (to match HawkEars) using the raven annotations\n",
    "# and some utility functions from opensoundscape\n",
    "from opensoundscape.annotations import BoxedAnnotations\n",
    "\n",
    "audio_and_raven_files = pd.read_csv(f\"{dataset_path}/audio_and_raven_files.csv\")\n",
    "# update the paths to where we have the audio and raven files stored\n",
    "audio_and_raven_files[\"audio\"] = audio_and_raven_files[\"audio\"].apply(\n",
    "    lambda x: f\"{dataset_path}/{x}\"\n",
    ")\n",
    "audio_and_raven_files[\"raven\"] = audio_and_raven_files[\"raven\"].apply(\n",
    "    lambda x: f\"{dataset_path}/{x}\"\n",
    ")\n",
    "\n",
    "annotations = BoxedAnnotations.from_raven_files(\n",
    "    raven_files=audio_and_raven_files[\"raven\"],\n",
    "    audio_files=audio_and_raven_files[\"audio\"],\n",
    "    annotation_column=\"annotation\",\n",
    ")\n",
    "# generate labels for 5s clips, including any labels that overlap by at least 0.2 seconds\n",
    "labels = annotations.clip_labels(clip_duration=5, min_label_overlap=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect labels\n",
    "\n",
    "Count number of each annotation type: \n",
    "\n",
    "Note that the 'X' label is for when the annotator was uncertain about the identity of a call. Labels A-E denote distinct call types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    512\n",
       "E    128\n",
       "D     62\n",
       "B     24\n",
       "C     74\n",
       "X    118\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into training and validation data\n",
    "We'll just focus on class 'A', the call type with the most annotations. We'll randomly split the clips into training and validation data, acknowledging that this approach does not test the ability of the model to generalize. Since the samples in the training and validation sets could be adjascent 2-second audio clips, good performance could simply mean the model has memorized the training samples, and the validation set has very similar samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train, labels_val = sklearn.model_selection.train_test_split(labels[[\"A\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train shallow classifiers on embedding model outputs\n",
    "\n",
    "We'll train our classifiers on a small annotated dataset with HawkEars, Perch, and BirdNET as feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/SML161/.cache/torch/hub/kitzeslab_bioacoustics-model-zoo_0.11.0.dev1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from URL...\n",
      "File hgnet1.ckpt already exists; skipping download.\n",
      "Loading model from local checkpoint /Users/SML161/opensoundscape/docs/tutorials/hgnet1.ckpt...\n",
      "Downloading model from URL...\n",
      "File hgnet2.ckpt already exists; skipping download.\n",
      "Loading model from local checkpoint /Users/SML161/opensoundscape/docs/tutorials/hgnet2.ckpt...\n",
      "Downloading model from URL...\n",
      "File hgnet3.ckpt already exists; skipping download.\n",
      "Loading model from local checkpoint /Users/SML161/opensoundscape/docs/tutorials/hgnet3.ckpt...\n",
      "Downloading model from URL...\n",
      "File hgnet4.ckpt already exists; skipping download.\n",
      "Loading model from local checkpoint /Users/SML161/opensoundscape/docs/tutorials/hgnet4.ckpt...\n",
      "Downloading model from URL...\n",
      "File hgnet5.ckpt already exists; skipping download.\n",
      "Loading model from local checkpoint /Users/SML161/opensoundscape/docs/tutorials/hgnet5.ckpt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/ml/cnn.py:621: UserWarning: \n",
      "                    This architecture is not listed in opensoundscape.ml.cnn_architectures.ARCH_DICT.\n",
      "                    It will not be available for loading after saving the model with .save() (unless using pickle=True). \n",
      "                    To make it re-loadable, define a function that generates the architecture from arguments: (n_classes, n_channels) \n",
      "                    then use opensoundscape.ml.cnn_architectures.register_architecture() to register the generating function.\n",
      "\n",
      "                    The function can also set the returned object's .constructor_name to the registered string key in ARCH_DICT\n",
      "                    to avoid this warning and ensure it is reloaded correctly by opensoundscape.ml.load_model().\n",
      "\n",
      "                    See opensoundscape.ml.cnn_architectures module for examples of constructor functions\n",
      "                    \n",
      "  warnings.warn(\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/cnn.py:645: UserWarning: Failed to detect expected # input channels of this architecture.Make sure your architecture expects the number of channels equal to `channels` argument 1). Pytorch architectures generally expect 3 channels by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import bioacoustics_model_zoo as bmz\n",
    "\n",
    "hawk = bmz.HawkEars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a shallow classifier that we'll train with embeddings as inputs. The input size needs to match the size of the embeddings produced by our embedding model. HawkEars embeddings are vectors of length 2048. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(\n",
    "    input_size=2048, output_size=labels_train.shape[1], hidden_layer_sizes=()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a single function that will embed the training and validation samples, then train the classifier.\n",
    "\n",
    "This will take a minute or two, since all of the samples need to be embedded with HawkEars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding the training samples without augmentation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11693d9294ff42f98896b60b69dc0ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding the validation samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e7c233f1984a2baad554a39bf2ab37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the classifier\n",
      "Epoch 100/1000, Loss: 0.5505968928337097, Val Loss: 0.551829993724823\n",
      "val AU ROC: 0.768\n",
      "val MAP: 0.768\n",
      "Epoch 200/1000, Loss: 0.503508448600769, Val Loss: 0.5115979313850403\n",
      "val AU ROC: 0.774\n",
      "val MAP: 0.774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(train_features, dtype=torch.float32, device=device)\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels, dtype=torch.float32, device=device)\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_features = torch.tensor(\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_labels = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/1000, Loss: 0.48121362924575806, Val Loss: 0.4945080280303955\n",
      "val AU ROC: 0.778\n",
      "val MAP: 0.778\n",
      "Epoch 400/1000, Loss: 0.46753495931625366, Val Loss: 0.4849015772342682\n",
      "val AU ROC: 0.783\n",
      "val MAP: 0.783\n",
      "Epoch 500/1000, Loss: 0.4575219750404358, Val Loss: 0.47844940423965454\n",
      "val AU ROC: 0.787\n",
      "val MAP: 0.787\n",
      "Epoch 600/1000, Loss: 0.4493757486343384, Val Loss: 0.4737447500228882\n",
      "val AU ROC: 0.790\n",
      "val MAP: 0.790\n",
      "Epoch 700/1000, Loss: 0.4423135221004486, Val Loss: 0.47020459175109863\n",
      "val AU ROC: 0.792\n",
      "val MAP: 0.792\n",
      "Epoch 800/1000, Loss: 0.43594419956207275, Val Loss: 0.4675188660621643\n",
      "val AU ROC: 0.793\n",
      "val MAP: 0.793\n",
      "Epoch 900/1000, Loss: 0.4300559461116791, Val Loss: 0.46549251675605774\n",
      "val AU ROC: 0.795\n",
      "val MAP: 0.795\n",
      "Epoch 1000/1000, Loss: 0.4245266020298004, Val Loss: 0.46399158239364624\n",
      "val AU ROC: 0.796\n",
      "val MAP: 0.796\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "emb_train, label_train, emb_val, label_val = fit_classifier_on_embeddings(\n",
    "    embedding_model=hawk,\n",
    "    classifier_model=clf,\n",
    "    train_df=labels_train,\n",
    "    validation_df=labels_val,\n",
    "    steps=1000,\n",
    "    embedding_batch_size=128,\n",
    "    embedding_num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's evaluate our shallow classifier on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average precision score: 0.7960448728167171\n",
      "area under ROC: 0.8267195767195767\n"
     ]
    }
   ],
   "source": [
    "# predict classes with the shallow classifier starting from the embeddings\n",
    "preds = clf(emb_val.to(torch.device(\"cpu\"))).detach().numpy()\n",
    "\n",
    "# evaluate with threshold agnostic metrics: MAP and ROC AUC\n",
    "print(\n",
    "    f\"average precision score: {average_precision_score(label_val,preds,average=None)}\"\n",
    ")\n",
    "print(f\"area under ROC: {roc_auc_score(label_val,preds,average=None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to visualize the performance, let's plot histograms of classifier logit scores for positive and negative samples\n",
    "\n",
    "it shows that precision is good for scores above >0 (few negatives get high scores), but recall is only moderate (many positive samples get low scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/miniconda3/envs/opso_dev/lib/python3.9/site-packages/matplotlib_inline/config.py:68: DeprecationWarning: InlineBackend._figure_format_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_format_changed(self, name, old, new):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c6d03f40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 428,
       "width": 1209
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(preds[label_val == 1], bins=20, alpha=0.5, label=\"positives\")\n",
    "plt.hist(preds[label_val == 0], bins=20, alpha=0.5, label=\"negatives\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can embed the training and validation sets first, then train as many different variants as we want.\n",
    "\n",
    "(note that the `fit_classifier_on_embeddings` returns the embeddings on the training and validation set, so if you've already run that function you don't need to re-generate the embeddings)\n",
    "\n",
    "Generally, embedding may take a while for large datasets, but training the shallow classifier will be very fast because the network is small and there is no preprocessing or data loading. \n",
    "\n",
    "For example, here we compare fitting classifiers with one or two hidden layers on the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the lines below to generate training and validation set embeddings, if you don't have them from the previous cells\n",
    "# emb_train = hawk.embed(labels_train, return_dfs=False, batch_size=128, num_workers=num_workers)\n",
    "# emb_val = hawk.embed(labels_val, return_dfs=False, batch_size=128, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(train_features, dtype=torch.float32, device=device)\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_features = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.43477728962898254, Val Loss: 0.4654878079891205\n",
      "val AU ROC: 0.795\n",
      "val MAP: 0.795\n",
      "Epoch 200/1000, Loss: 0.36520084738731384, Val Loss: 0.47021692991256714\n",
      "val AU ROC: 0.791\n",
      "val MAP: 0.791\n",
      "Epoch 300/1000, Loss: 0.2947371006011963, Val Loss: 0.5176641345024109\n",
      "val AU ROC: 0.779\n",
      "val MAP: 0.779\n",
      "Epoch 400/1000, Loss: 0.22931015491485596, Val Loss: 0.5908443927764893\n",
      "val AU ROC: 0.764\n",
      "val MAP: 0.764\n",
      "Epoch 500/1000, Loss: 0.17564012110233307, Val Loss: 0.6850444078445435\n",
      "val AU ROC: 0.753\n",
      "val MAP: 0.753\n",
      "Epoch 600/1000, Loss: 0.13606344163417816, Val Loss: 0.7922593355178833\n",
      "val AU ROC: 0.744\n",
      "val MAP: 0.744\n",
      "Epoch 700/1000, Loss: 0.10693871974945068, Val Loss: 0.9060013294219971\n",
      "val AU ROC: 0.739\n",
      "val MAP: 0.739\n",
      "Epoch 800/1000, Loss: 0.08502118289470673, Val Loss: 1.0221915245056152\n",
      "val AU ROC: 0.734\n",
      "val MAP: 0.734\n",
      "Epoch 900/1000, Loss: 0.0681585744023323, Val Loss: 1.139981746673584\n",
      "val AU ROC: 0.728\n",
      "val MAP: 0.728\n",
      "Epoch 1000/1000, Loss: 0.05483967810869217, Val Loss: 1.2582722902297974\n",
      "val AU ROC: 0.723\n",
      "val MAP: 0.723\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(train_features, dtype=torch.float32, device=device)\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_features = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.3648201525211334, Val Loss: 0.47713956236839294\n",
      "val AU ROC: 0.787\n",
      "val MAP: 0.787\n",
      "Epoch 200/1000, Loss: 0.20261050760746002, Val Loss: 0.6866857409477234\n",
      "val AU ROC: 0.750\n",
      "val MAP: 0.750\n",
      "Epoch 300/1000, Loss: 0.10805604606866837, Val Loss: 0.9950821399688721\n",
      "val AU ROC: 0.726\n",
      "val MAP: 0.726\n",
      "Epoch 400/1000, Loss: 0.05842690169811249, Val Loss: 1.3393805027008057\n",
      "val AU ROC: 0.715\n",
      "val MAP: 0.715\n",
      "Epoch 500/1000, Loss: 0.03197795897722244, Val Loss: 1.6808558702468872\n",
      "val AU ROC: 0.708\n",
      "val MAP: 0.708\n",
      "Epoch 600/1000, Loss: 0.017450671643018723, Val Loss: 1.995842695236206\n",
      "val AU ROC: 0.705\n",
      "val MAP: 0.705\n",
      "Epoch 700/1000, Loss: 0.010155580937862396, Val Loss: 2.267706871032715\n",
      "val AU ROC: 0.702\n",
      "val MAP: 0.702\n",
      "Epoch 800/1000, Loss: 0.006417952012270689, Val Loss: 2.4932973384857178\n",
      "val AU ROC: 0.701\n",
      "val MAP: 0.701\n",
      "Epoch 900/1000, Loss: 0.0043555074371397495, Val Loss: 2.6811463832855225\n",
      "val AU ROC: 0.699\n",
      "val MAP: 0.699\n",
      "Epoch 1000/1000, Loss: 0.0031238417141139507, Val Loss: 2.840564250946045\n",
      "val AU ROC: 0.698\n",
      "val MAP: 0.698\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# define classifier with one hidden layer, and fit\n",
    "classifier_model_1 = MLPClassifier(2048, 1, hidden_layer_sizes=(100,))\n",
    "classifier_model_1.fit(\n",
    "    emb_train, labels_train.values, emb_val, labels_val.values, steps=1000\n",
    ")\n",
    "\n",
    "# define classifier with two hidden layers, and fit\n",
    "classifier_model_2 = MLPClassifier(2048, 1, hidden_layer_sizes=(100, 100))\n",
    "classifier_model_2.fit(\n",
    "    emb_train, labels_train.values, emb_val, labels_val.values, steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier_model_1 area under ROC: 0.7374527588813303\n",
      "classifier_model_2 area under ROC: 0.7227891156462586\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "preds = classifier_model_1(emb_val)\n",
    "print(\n",
    "    f\"classifier_model_1 area under ROC: {roc_auc_score(labels_val.values,preds.detach().numpy(),average=None)}\"\n",
    ")\n",
    "\n",
    "preds = classifier_model_2(emb_val)\n",
    "print(\n",
    "    f\"classifier_model_2 area under ROC: {roc_auc_score(labels_val.values,preds.detach().numpy(),average=None)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train on variants of the embeddings generated with audio-space augmentations\n",
    "\n",
    "The `fit_classifier_on_embeddings` function supports generating variants of training samples with augmentation via the parameter `n_augmentation_variants`. The default 0 does not perform augentation. Specifying a positive integer tells the function to generate each sample n times using stochastic augmentation. The specific augmentations performed are defined by the embedding model's `.preprocessor`. \n",
    "\n",
    "We can also generate the augmented samples directly using `opensoundscape.ml.shallow_classifier.augmented_embed`, similarly to how we generated embeddings above then trained various models on them. Note that preprocessing and sample loading is repeated for each iteration of augmented data creation, so augmented_embed will take `n_augmentation_variants` times longer than embedding without augmentation. The benefit is that augmenting the audio samples before embedding tends to improve model performance more than simply augmenting the embeddings themselves (e.g. by adding random noise). \n",
    "\n",
    "For the sake of speed, we demonstrate augmented embedding here on only a subset of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b2230613ad43e2bce287effec5415d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from opensoundscape.ml.shallow_classifier import augmented_embed\n",
    "\n",
    "train_emb_aug, train_label_aug = augmented_embed(\n",
    "    hawk,\n",
    "    labels_train.sample(512),\n",
    "    batch_size=128,\n",
    "    num_workers=num_workers,\n",
    "    n_augmentation_variants=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we embed the validation set as normal, without any augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run if you don't already have emb_val from previous steps\n",
    "# emb_val = hawk.embed(labels_val, return_dfs=False, batch_size=128, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting the classifier on the augmented variants' embeddings looks the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(train_features, dtype=torch.float32, device=device)\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_labels = torch.tensor(train_labels, dtype=torch.float32, device=device)\n",
      "/Users/SML161/opensoundscape/opensoundscape/ml/shallow_classifier.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_features = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 200/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 300/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 400/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 500/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 600/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 700/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 800/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 900/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Epoch 1000/1000, Loss: nan, Val Loss: nan\n",
      "val AU ROC: nan\n",
      "val MAP: nan\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "classifier_model = MLPClassifier(2048, 1, hidden_layer_sizes=())\n",
    "quick_fit(\n",
    "    classifier_model,\n",
    "    train_emb_aug,\n",
    "    train_label_aug,\n",
    "    emb_val,\n",
    "    labels_val.values,\n",
    "    steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/265wdp1n0bn_r85dh3pp95fh0000gq/T/ipykernel_76442/1177080571.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds = classifier_model(torch.tensor(emb_val).to(torch.device('cpu')))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m preds \u001b[38;5;241m=\u001b[39m classifier_model(torch\u001b[38;5;241m.\u001b[39mtensor(emb_val)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mroc_auc_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/opso_dev/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/opso_dev/lib/python3.9/site-packages/sklearn/metrics/_ranking.py:619\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    617\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    618\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 619\u001b[0m y_score \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    622\u001b[0m     y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_score\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    623\u001b[0m ):\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;66;03m# do not support partial ROC computation for multiclass\u001b[39;00m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_fpr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_fpr \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/opso_dev/lib/python3.9/site-packages/sklearn/utils/validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1061\u001b[0m     )\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/opso_dev/lib/python3.9/site-packages/sklearn/utils/validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/opso_dev/lib/python3.9/site-packages/sklearn/utils/validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "preds = classifier_model(torch.tensor(emb_val).to(torch.device(\"cpu\")))\n",
    "roc_auc_score(labels_val.values, preds.detach().numpy(), average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit SKLearn Classifiers on embeddings\n",
    "scikit-learn provides various classification algorithms as alternatives to the MLPClassifier implemented in OpenSoundscape via PyTorch. It's straightforward to fit any sklearn model on embeddings: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.804932975150205"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# initialize a random forest class from sklearn\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# fit the model on training set embeddings\n",
    "rf.fit(emb_train, labels_train.values[:, 0])\n",
    "\n",
    "# evaluate on the validation set\n",
    "preds = rf.predict(emb_val)\n",
    "roc_auc_score(labels_val.values, preds, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's another example with K nearest neighbors classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8058895123845515"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize classifier\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "# fit on training set embeddings\n",
    "knc.fit(emb_train, labels_train.values[:, 0])\n",
    "\n",
    "# evaluate on validation set\n",
    "preds = knc.predict(emb_val)\n",
    "roc_auc_score(labels_val.values, preds, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a classifier that is a layer in an exisisting OpenSoundscape model\n",
    "\n",
    "If you have a fully connected layer at the end of an existing OpenSoundscape model, training that layer works similarly to training a separate MLPClassifier object. We can use the `quick_fit` function to train the layer on pre-generated embeddings (output of previous network layer) to avoid the slow-down associated with preprocessing samples for every training step. \n",
    "\n",
    "For example, let's load up a CNN trained in OpenSoundscape from the model zoo. This CNN was trained to detect the A and B call types of Rana Sierrae vocalizations. It has a resnet18 architecture that ends with a fully connected classifier layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/SML161/.cache/torch/hub/kitzeslab_bioacoustics-model-zoo_0.11.0.dev1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db27633632284064902119a6a4753491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648711dec9aa4f56bc9ced30406f77c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bioacoustics_model_zoo as bmz\n",
    "\n",
    "rana_sierrae_cnn = bmz.load(\"rana_sierrae_cnn\")\n",
    "train_emb = rana_sierrae_cnn.embed(\n",
    "    labels_train, return_dfs=False, batch_size=128, num_workers=num_workers\n",
    ")\n",
    "val_emb = rana_sierrae_cnn.embed(\n",
    "    labels_val, return_dfs=False, batch_size=128, num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: 0.41220611333847046, Val Loss: 0.4730120599269867\n",
      "val AU ROC: 0.777\n",
      "val MAP: 0.777\n",
      "Epoch 200/1000, Loss: 0.3850261867046356, Val Loss: 0.47698184847831726\n",
      "val AU ROC: 0.780\n",
      "val MAP: 0.780\n",
      "Epoch 300/1000, Loss: 0.3647022247314453, Val Loss: 0.47917938232421875\n",
      "val AU ROC: 0.781\n",
      "val MAP: 0.781\n",
      "Epoch 400/1000, Loss: 0.3479377031326294, Val Loss: 0.4816058874130249\n",
      "val AU ROC: 0.783\n",
      "val MAP: 0.783\n",
      "Epoch 500/1000, Loss: 0.333607017993927, Val Loss: 0.4848082959651947\n",
      "val AU ROC: 0.784\n",
      "val MAP: 0.784\n",
      "Epoch 600/1000, Loss: 0.3210768401622772, Val Loss: 0.4887297451496124\n",
      "val AU ROC: 0.785\n",
      "val MAP: 0.785\n",
      "Epoch 700/1000, Loss: 0.30992019176483154, Val Loss: 0.4933810532093048\n",
      "val AU ROC: 0.784\n",
      "val MAP: 0.784\n",
      "Epoch 800/1000, Loss: 0.2998389005661011, Val Loss: 0.4988059103488922\n",
      "val AU ROC: 0.782\n",
      "val MAP: 0.782\n",
      "Epoch 900/1000, Loss: 0.290620356798172, Val Loss: 0.5050168037414551\n",
      "val AU ROC: 0.783\n",
      "val MAP: 0.783\n",
      "Epoch 1000/1000, Loss: 0.28210970759391785, Val Loss: 0.5119875073432922\n",
      "val AU ROC: 0.783\n",
      "val MAP: 0.783\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# modify the last layer of the CNN to have a single output for the class 'A'\n",
    "rana_sierrae_cnn.change_classes([\"A\"])  # replace fc layer with 1-output layer\n",
    "\n",
    "# fit the fc layer within the opso CNN by passing the layer to the `quick_fit` function\n",
    "quick_fit(\n",
    "    rana_sierrae_cnn.network.fc,\n",
    "    train_emb,\n",
    "    labels_train.values,\n",
    "    val_emb,\n",
    "    labels_val.values,\n",
    "    steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2dcfd9ebecd4d7da503e162fe5b6862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8321995464852607"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate:\n",
    "\n",
    "# can use regular prediction since we modified the in-network classifier\n",
    "# but this will be slower than just running the fc layer on the embeddings, since it requires\n",
    "# preprocessing and running the entire CNN architecture forward pass\n",
    "preds = rana_sierrae_cnn.predict(labels_val, batch_size=128)\n",
    "roc_auc_score(labels_val.values, preds, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could equivalently get the prediciton by passing the embeddings through the trained fc layer. The outputs should be numerically similar to prediction starting from the audio clips:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds2 = (\n",
    "    rana_sierrae_cnn.network.fc(torch.tensor(val_emb).to(torch.device(\"mps\")))\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .numpy()\n",
    ")\n",
    "np.max(np.abs((preds.values - preds2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also replace the cnn's `.fc` layer with an MLPClassifier object, if we want the classifier to be more than one fully-connected layer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opso_dev",
   "language": "python",
   "name": "opso_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
