{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas \n",
    "import opensoundscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities that should probably go in another module\n",
    "# generalized cross-correlation\n",
    "def gcc(x, y, max_delay_samples=None, filter='phat', epsilon = 0.01):\n",
    "    \"\"\"\n",
    "    GCC implementation based on Knapp and Carter - code adapted from\n",
    "    github.com/axeber01/ngcc\n",
    "    Args:\n",
    "        x: 1d numpy array of audio samples\n",
    "        y: 1d numpy array of audio samples\n",
    "        max_delay_samples: maximum possible delay between the 2 signals in max_delay_samples\n",
    "        filter: which filter to use in the gcc.\n",
    "            'phat' - Phase transform,\n",
    "            'roth',\n",
    "            'scot' - Smoothed Coherence Transform,\n",
    "            'ht' - Hannan and Thomson\n",
    "        epsilon = used to ensure denominator is non-zero.\n",
    "    \"\"\"\n",
    "    n = x.shape[0] + y.shape[0]\n",
    "\n",
    "    # Generalized Cross Correlation Phase Transform\n",
    "    X = np.fft.rfft(x, n=n)\n",
    "    Y = np.fft.rfft(y, n=n)\n",
    "    Gxy = X * np.conj(Y)\n",
    "\n",
    "    if filter == 'phat':\n",
    "        phi = 1 / (np.abs(Gxy) + epsilon)\n",
    "\n",
    "    elif filter == 'roth':\n",
    "        phi = 1 / (X * torch.conj(X) + epsilon)\n",
    "\n",
    "    elif filter == 'scot':\n",
    "        Gxx = X * np.conj(X)\n",
    "        Gyy = Y * np.conj(Y)\n",
    "        phi = 1 / (np.sqrt(X * Y) + epsilon)\n",
    "\n",
    "    elif filter == 'ht':\n",
    "        Gxx = X * np.conj(X)\n",
    "        Gyy = Y * np.conj(Y)\n",
    "        gamma = Gxy / np.sqrt(Gxx * Gxy)\n",
    "        phi = np.abs(gamma)**2 / (np.abs(Gxy)\n",
    "                                     * (1 - gamma)**2 + epsilon)\n",
    "    elif filter == 'cc':\n",
    "        phi = 1.0\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported filter. Must be one of: 'ht', 'phat', 'roth','scot'\")\n",
    "\n",
    "\n",
    "    #set the max delay in number of samples\n",
    "    if max_delay_samples:\n",
    "        max_delay_samples = np.minimum(max_delay_samples, int(n/2))\n",
    "    else:\n",
    "        max_delay_samples = int(n/2)\n",
    "\n",
    "    cc = np.fft.irfft(Gxy * phi, n)\n",
    "\n",
    "    return cc\n",
    "\n",
    "def gillette_localize(\n",
    "        receivers=list,\n",
    "        delays=list,\n",
    "        temp=20,\n",
    "        m = [0],\n",
    "        exact=True,\n",
    "        summary=False,\n",
    "        confint=False,\n",
    "        alpha=0.05,\n",
    "        td_error=False,\n",
    "        total_td_error=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Calculate the estimated location of a sound's source using the\n",
    "        algorithm laid out in Gillette and Silverman (2008)\n",
    "        Args:\n",
    "            receivers: A numpy array of coordinates for microphones used to\n",
    "            record the sound. The number of microphones needed should\n",
    "            be two more than the dimensions being localized in. The\n",
    "            first row will be treated as a reference point for the\n",
    "            algorithm.\n",
    "            tdoa: A list of time delays. Each entry should be the time\n",
    "            delay for the corresponding item in the receivers list\n",
    "            (i.e. the first item is the delay for the first receiver).\n",
    "            The first item in this list should be 0, with all other\n",
    "            entries centered around that.\n",
    "            temp: ambient temperature in Celsius. Defaults to 20.\n",
    "            exact: computes an exact solution if True, computes estimates\n",
    "            with uncertainty if false. Defaults to True\n",
    "            summary: displays a summary of the estimates if True. Defaults\n",
    "            to false.\n",
    "            confint: outputs confidence intervals for the estimated\n",
    "            coordinates if true. Defaults to false.\n",
    "            alpha: Determines confidence level of the confidence intervals.\n",
    "            Defaults to 0.05.\n",
    "            m: the index of the reference mic. Defaults to 0.\n",
    "            td_error: Computes the expected time delay from the estimated\n",
    "            source location, centered around the reference mic, for each\n",
    "            microphone.\n",
    "            total_td_error: Computes the euclidean norm of the errors\n",
    "            provided by td_error.\n",
    "        Returns:\n",
    "            an array with the estimated coordinates and the estimated\n",
    "            distance from the reference mic. (One reference mic and two\n",
    "            additional mics, this is a 2 item array containing an estima\n",
    "            -ted x coordinate and a distance.)\n",
    "        \"\"\"\n",
    "        import opensoundscape.localization as loc\n",
    "        import statsmodels.api as sm\n",
    "        C = loc.calc_speed_of_sound(temperature=20)\n",
    "        # Compile know receiver locations and distance delays into an output vector\n",
    "        out_knowns = []\n",
    "        in_knowns = np.zeros(((len(receivers) - len(m))*len(m), 2 + len(m)))\n",
    "        toa = np.array(delays)\n",
    "        r = 0\n",
    "        out_knowns = []\n",
    "        for k in range(len(m)):\n",
    "            tdoa = toa - toa[m[k]] # Use the speed of sound to convert time delays to \"distance delays\"\n",
    "            diffs = []\n",
    "            for delay in tdoa:\n",
    "                diffs.append(float(delay * loc.calc_speed_of_sound(20)))\n",
    "            for i in range(len(receivers)):\n",
    "                        if i in m:\n",
    "                            continue\n",
    "                        else:\n",
    "                            w = diffs[i] ** 2\n",
    "                            for j in range(len(receivers[i])):\n",
    "                                w = w - receivers[i][j] ** 2 + receivers[m[k]][j] ** 2\n",
    "                            w = w / 2\n",
    "                            out_knowns.append(w)\n",
    "            for i in range(len(receivers)):\n",
    "                    if i in m:\n",
    "                        continue\n",
    "                    else:\n",
    "                        q = 0\n",
    "                        for j in range(len(receivers[i])):\n",
    "                            z = receivers[m[k]][j] - receivers[i][j]\n",
    "                            in_knowns[r][q] = z\n",
    "                            q += 1\n",
    "                        in_knowns[r][q+k] = diffs[i]\n",
    "                        r += 1\n",
    "                        continue\n",
    "\n",
    "            # Using least squares, compute the final estimated location of source\n",
    "        location = sm.OLS(out_knowns, in_knowns).fit()\n",
    "        return location.params, location.summary(alpha = alpha), location.conf_int(alpha = alpha)\n",
    "        if summary == True:\n",
    "            return location.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a class that we will use to contain a model object, list of files and thresholds\n",
    "# this will be called Localizer\n",
    "# we will use this class for localizing sound sources from synchronized audio files\n",
    "class Localizer:\n",
    "    def __init__(self, model, files, aru_coords, sample_rate, min_number_of_arus, thresholds = None, predictions = None, bandpass_range = None, max_delay = None, cc_threshold = 0):\n",
    "        # initialize the class\n",
    "        # model is a trained opensoundscape model\n",
    "        # files is a list of synchronized audio files\n",
    "        # aru_coords is a dictionary of aru coordinates, with key aru file path, and value (x,y) coordinates\n",
    "        # thresholds is a dictionary of thresholds for each class\n",
    "        # predictions is a pandas dataframe of predictions\n",
    "        self.model = model\n",
    "        self.files = files\n",
    "        self.aru_coords = aru_coords\n",
    "        self.thresholds = thresholds\n",
    "        self.SAMPLE_RATE = sample_rate\n",
    "        self.min_number_of_arus = min_number_of_arus\n",
    "        self.predictions = predictions\n",
    "        self.bandpass_range = bandpass_range\n",
    "        self.max_delay = max_delay\n",
    "        self.cc_threshold = cc_threshold\n",
    "\n",
    "        # initialize the below intermediates as None. #TODO: work out how to do this correctly\n",
    "        self.detections = None\n",
    "        self.cross_correlations = None\n",
    "        self.filtered_cross_correlations = None\n",
    "    \n",
    "    def get_predictions(self):\n",
    "        # get CNN predictions from synchronized audio files\n",
    "        # return a pandas dataframe with the results\n",
    "        if self.predictions is None:\n",
    "            self.predictions = self.model.predict(self.files, activation_layer = None)\n",
    "        else:\n",
    "            raise UserWarning(\"Predictions already exist - set predictions to None if you want to re-run predictions\")\n",
    "        return self.predictions\n",
    "\n",
    "    def threshold_predictions(self):\n",
    "        # use a set of thresholds to filter the predictions\n",
    "        if self.predictions is None:\n",
    "            print(\"No predictions exist - running predictions\")\n",
    "            self.get_predictions()\n",
    "        all_sp_detections = []\n",
    "        for species in self.predictions.columns:\n",
    "            df = predictions_df.loc[:,[species]] #must be a dataframe\n",
    "            detections = Localizer._get_detections(df, cnn_score_threshold = self.thresholds[species])\n",
    "            grouped_detections = Localizer._group_detections(detections, self.aru_coords, self.max_distance_between_recorders)\n",
    "            all_sp_detections.append(grouped_detections)\n",
    "        detections_df = pandas.concat(all_sp_detections, axis=1)\n",
    "        self.detections = detections_df\n",
    "        return detections_df\n",
    "\n",
    "    def cross_correlate(self):\n",
    "        # cross correlate the predictions\n",
    "        # return a pandas dataframe with the results      \n",
    "        if self.bandpass_range is None:\n",
    "            raise UserWarning(\"No bandpass range specified\")\n",
    "        if self.max_delay is None:\n",
    "            raise UserWarning(\"No max delay specified\")\n",
    "        if self.detections is None:\n",
    "            print(\"No detections exist - running threshold_predictions\")\n",
    "            self.threshold_predictions()\n",
    "        # get the cross-correlations\n",
    "        all_ccs = []\n",
    "        all_tds = []\n",
    "        for index, row in self.detections.iterrows():\n",
    "            cc,td = Localizer._get_cross_correlations(\n",
    "                reference_file=row[\"reference_file\"],\n",
    "                other_files=row[\"other_files\"],\n",
    "                start_time=row[\"time\"][0],\n",
    "                end_time=row[\"time\"][1],\n",
    "                bandpass_range=self.bandpass_ranges[species],\n",
    "                max_delay=self.max_delay,\n",
    "                SAMPLE_RATE=44100,\n",
    "                )\n",
    "            all_ccs.append(cc)\n",
    "            all_tds.append(td)\n",
    "        self.cross_correlations = self.detections.copy()\n",
    "        self.cross_correlations[\"cross_correlations\"] = all_ccs\n",
    "        self.cross_correlations[\"time_delays\"] = all_tds\n",
    "        return self.cross_correlations\n",
    "    \n",
    "    def filter_cross_correlations(self):\n",
    "        # filter the cross correlations\n",
    "        # return a pandas dataframe with the results\n",
    "        if self.cross_correlations is None:\n",
    "            print(\"No cross correlations exist - running cross_correlate\")\n",
    "            self.cross_correlate()\n",
    "        # filter the cross-correlations \n",
    "        above_threshold = [cc > self.cc_threshold for cc in self.cross_correlations[\"cross_correlations\"]]\n",
    "    \n",
    "        n_before = len(self.cross_correlations) # number of rows before filtering\n",
    "\n",
    "        filtered_ccs = []\n",
    "        filtered_files = []\n",
    "        filtered_tdoas = []\n",
    "        for i in range(len(self.cross_correlations)):\n",
    "            mask = above_threshold[i]\n",
    "            cc = self.cross_correlations[\"cross_correlations\"].iloc[i]\n",
    "            other_files = np.array(self.cross_correlations[\"other_files\"].iloc[i])\n",
    "            tdoa = np.array(self.cross_correlations[\"time_delays\"].iloc[i])\n",
    "\n",
    "            filtered_ccs.append(cc[mask])\n",
    "            filtered_files.append(other_files[mask])\n",
    "            filtered_tdoas.append(tdoa[mask])\n",
    "        \n",
    "        filtered_cross_correlations = self.cross_correlations.copy()\n",
    "\n",
    "        filtered_cross_correlations[\"cross_correlations\"] = filtered_ccs\n",
    "        filtered_cross_correlations[\"other_files\"] = filtered_files\n",
    "        filtered_cross_correlations[\"time_delays\"] = filtered_tdoas\n",
    "\n",
    "        # Filter by the cc scores. If less than min_number_of_ARUs have cc_score above threshold, drop them.\n",
    "        ccs = [np.array(scores) for scores in filtered_cross_correlations[\"cross_correlations\"]]\n",
    "        num_ccs_above_threshold = [sum(a > self.cc_threshold) for a in ccs]\n",
    "        mask = np.array(num_ccs_above_threshold) >= self.min_number_of_ARUs - 1\n",
    "        filtered_cross_correlations = filtered_cross_correlations[mask]\n",
    "\n",
    "        n_after = len(filtered_cross_correlations) # number of rows after filtering\n",
    "        print(f\"{n_before - n_after} rows deleted\")\n",
    "        self.filtered_cross_correlations = filtered_cross_correlations\n",
    "        return filtered_cross_correlations\n",
    "\n",
    "\n",
    "    def localize(algorithm = \"gillette\"):\n",
    "        # localize the detections\n",
    "        # return a pandas dataframe with the results\n",
    "        #TODO: make work for 3d \n",
    "\n",
    "        localized = self.filtered_cross_correlations.copy()\n",
    "        locations = []\n",
    "        if  self.filtered_cross_correlations is None:\n",
    "            print(\"No filtered cross_correlations exist - running filter_cross_correlations\")\n",
    "            self.filter_cross_correlations()\n",
    "        if algorithm == \"gillette\":\n",
    "            # localize using gillette\n",
    "            \n",
    "            for index, row in filtered_detections.iterrows():\n",
    "                reference = row[\"reference_file\"]\n",
    "                others = row[\"other_files\"]\n",
    "                reference_coords = aru_coords_df.loc[reference]\n",
    "                others_coords = [aru_coords_df.loc[i] for i in others]\n",
    "                all_coords = [reference_coords] + others_coords\n",
    "                # add 0 tdoa for reference receiver\n",
    "                delays = np.insert(row[\"time_delays\"], 0, 0)\n",
    "\n",
    "            location,_,_=gillette_localize(all_coords, delays)\n",
    "            locations.append(location)\n",
    "            filtered_detections[\"predicted_x\"] = [locations[i][0] for i in range(len(locations))]\n",
    "            filtered_detections[\"predicted_y\"] = [locations[i][1] for i in range(len(locations))]\n",
    "        elif algorithm == \"soundfinder\":\n",
    " \n",
    "            for index, row in self.filtered_cross_correlations.iterrows():\n",
    "                reference = row[\"reference_file\"]\n",
    "                others = row[\"other_files\"]\n",
    "                reference_coords = self.aru_coords.loc[reference]\n",
    "                others_coords = [self.aru_coords.loc[i] for i in others]\n",
    "                all_coords = [reference_coords] + others_coords\n",
    "                # add 0 tdoa for reference receiver\n",
    "                delays = np.insert(row[\"time_delays\"], 0, 0)\n",
    "\n",
    "                location = soundfinder(all_coords, delays)\n",
    "                locations.append(location)\n",
    "            localized[\"predicted_x\"] = [locations[i][0] for i in range(len(locations))]\n",
    "            localized[\"predicted_y\"] = [locations[i][1] for i in range(len(locations))]\n",
    "            localized[\"pseudorange_error\"] = [locations[i][2] for i in range(len(locations))]\n",
    "        else:\n",
    "            raise UserWarning(\"Algorithm not recognized\")\n",
    "\n",
    "    def _get_cross_correlations(\n",
    "        reference_file,\n",
    "        other_files,\n",
    "        start_time,\n",
    "        end_time,\n",
    "        bandpass_range,\n",
    "        max_delay,\n",
    "        SAMPLE_RATE):\n",
    "        \"\"\"\n",
    "        Gets the maximal cross correlations and the time-delay (in s) corresponding to that cross correlation between\n",
    "        the reference_file and other_files. Setting max_delay ensures that only cross-correlations\n",
    "        +/- a certain time-delay are returned. i.e if a sound can be a maximum of +/-\n",
    "        ----\n",
    "        args:\n",
    "            reference_file: Path to reference file.\n",
    "            other_files: List of paths to the other files which will be cross-correlated against reference_file\n",
    "            start_time: start of time segment (in seconds) to be cross-correlated\n",
    "            end_time: end of time segment (in seconds) to be cross-correlated.\n",
    "            bandpass_range: [lower, higher] of bandpass range.\n",
    "            max_delay: the maximum time (in seconds) to return cross_correlations for. i.e. if the best cross correlation\n",
    "                        occurs for a time-delay greater than max_delay, the function will not return it, instead it will return\n",
    "                        the maximal cross correlation within +/- max_delay\n",
    "            SAMPLE_RATE: the sampling rate of the audio.\n",
    "        returns:\n",
    "            ccs: list of maximal cross-correlations for each pair of files.\n",
    "            time_differences: list of time differences (in seconds) that yield the maximal cross-correlation.\n",
    "        \"\"\"\n",
    "        lower = min(bandpass_range)\n",
    "        higher = max(bandpass_range)\n",
    "\n",
    "        reference_audio = Audio.from_file(\n",
    "            reference_file, offset=start_time, duration=end_time - start_time\n",
    "        ).bandpass(lower, higher, order=9)\n",
    "        other_audio = [\n",
    "            Audio.from_file(i, offset=start_time, duration=end_time - start_time).bandpass(\n",
    "                lower, higher, order=9\n",
    "            )\n",
    "            for i in other_files\n",
    "        ]\n",
    "\n",
    "        max_lag = int(\n",
    "            max_delay * SAMPLE_RATE\n",
    "        )  # Convert max_delay (in s) to max_lag in samples\n",
    "\n",
    "        ccs = np.zeros(len(other_audio))\n",
    "        time_difference = np.zeros(len(other_audio))\n",
    "        for index, audio_object in enumerate(other_audio):\n",
    "            ff = reference_audio.samples\n",
    "            sf = audio_object.samples\n",
    "\n",
    "            # TODO: Normalize these, so cross-correlation will return values -1<cc<1\n",
    "            # TODO: verify this makes sense, could there be some floating point issues with this? Is it the right kind\n",
    "            # of normalization\n",
    "            ff = ff / np.std(ff)\n",
    "            sf = sf / np.std(sf)\n",
    "\n",
    "            cc = correlate(ff, sf, mode=\"same\")  # correlations are per sample\n",
    "            cc /= min(len(ff), len(sf))\n",
    "            lags = correlation_lags(ff.size, sf.size, mode=\"same\")\n",
    "\n",
    "            # slice cc and lags, so we only look at cross_correlations that are between -max_lag and +max_lag\n",
    "            lower_limit = int(len(cc) / 2 - max_lag)\n",
    "            upper_limit = int(len(cc) / 2 + max_lag)\n",
    "\n",
    "            cc = cc[lower_limit:upper_limit]\n",
    "            lags = lags[lower_limit:upper_limit]\n",
    "\n",
    "            # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "            max_cc = np.max(cc)\n",
    "            lag = -lags[\n",
    "                np.argmax(cc)\n",
    "            ]  # in ties (>2 ccs with same max value), argmax returns the first.\n",
    "            time_difference[index] = lag\n",
    "            ccs[index] = max_cc\n",
    "        time_difference = [i / SAMPLE_RATE for i in time_difference]\n",
    "\n",
    "        return ccs, time_difference\n",
    "    \n",
    "    def _get_detections(predictions_df, cnn_score_threshold):\n",
    "        \"\"\"\n",
    "        Takes the predictions_df of CNN scores, chooses only detections > cnn_score_threshold\n",
    "        and outputs a dictionary of times at which events were detected, and the ARU files they were detected in.\n",
    "        args:\n",
    "            predictions_array: a dataframe with multi-index of (file, start, end) with a column that is values for model predictions.\n",
    "            cnn_score_threshold: the minimum CNN score needed for a time-window to be considered a detection.\n",
    "        returns:\n",
    "            A dictionary of predictions, with key (start_time, end_time), and value list of files with detection triggered\n",
    "            e.g. {(0.0,2.0): [ARU_0.mp3. ARU_1.mp3]}\n",
    "        \"\"\"\n",
    "        # get the detections from the predictions\n",
    "        # Threshold the scores to above cnn_score_threshold\n",
    "        booleans = predictions_df.loc[:, :, :] > cnn_score_threshold # find rows above threshold\n",
    "        indices = booleans[booleans].dropna().index # choose just those rows. dropna required to drop the others\n",
    "        recorders = indices.get_level_values(0) # get the list of recorders out of the multi-index\n",
    "        indices = indices.droplevel(level=0) # drop the recorders\n",
    "\n",
    "        dataframe = pd.DataFrame(data=recorders, index=indices) # df with index (start_time, end_time)\n",
    "        dataframe = dataframe.sort_index() # done to ensure speed-up and not get performancewarning\n",
    "        recorders_list = []\n",
    "        for idx in dataframe.index.unique():\n",
    "            recorders_in_time = dataframe.loc[idx].values\n",
    "            recorders_in_time = [i[0] for i in recorders_in_time] # to get recorder path string out of numpy array\n",
    "            recorders_list.append(recorders_in_time)\n",
    "        return dict(zip(dataframe.index.unique(), recorders_list))\n",
    "\n",
    "    def _group_detections(detections, aru_coords, max_distance_between_recorders):\n",
    "        \"\"\"\n",
    "        Takes the detections dictionary and groups detections that are within max_distance_between_recorders of each other.\n",
    "        args:\n",
    "            detections: a dictionary of detections, with key (start_time, end_time), and value list of files with detection triggered\n",
    "            aru_coords: a dictionary of aru coordinates, with key aru file path, and value (x,y) coordinates\n",
    "            max_distance_between_recorders: the maximum distance between recorders to consider a detection as a single event\n",
    "        returns:\n",
    "            A dictionary of grouped detections, with key (start_time, end_time), and value list of files with detection triggered\n",
    "            e.g. {(0.0,2.0): [ARU_0.mp3. ARU_1.mp3]}\n",
    "        \"\"\"\n",
    "        # group detections that are within max_distance_between_recorders of each other\n",
    "        # return a dictionary of grouped detections\n",
    "        # get the coordinates of the recorders\n",
    "        # get the distance between recorders\n",
    "        # if the distance is less than max_distance_between_recorders, group the detections\n",
    "        from itertools import product\n",
    "\n",
    "        # Group recorders based on being within < max_distance_between_recorders.\n",
    "        # recorders_in_distance is dictionary in\n",
    "        # form {ARU_0.mp3: [ARU_1.mp3, ARU_2.mp3...] for all recorders within max_distance_between_recorders }\n",
    "        recorders_in_distance = dict()\n",
    "\n",
    "        aru_files = aru_coords_df.index\n",
    "        for aru in aru_files:  # loop over the aru files\n",
    "            pos_aru = np.array(aru_coords_df.loc[aru])\n",
    "            other_arus = np.array(aru_coords_df)\n",
    "            distances = other_arus - pos_aru\n",
    "            euclid_distances = [np.linalg.norm(d) for d in distances]\n",
    "\n",
    "            mask = [\n",
    "                0 < i <= max_distance_between_recorders for i in euclid_distances\n",
    "            ]  # boolean mask\n",
    "            recorders_in_distance[aru] = list(aru_files[mask])\n",
    "\n",
    "        times = []\n",
    "        reference_files = []\n",
    "        other_files = []\n",
    "\n",
    "        for time_segment in detections.keys():  # iterate through all the time-segments\n",
    "            for file in detections[\n",
    "                time_segment\n",
    "            ]:  # iterate through each file with a call detected in this time-segment\n",
    "                reference = file  # set this file to be reference\n",
    "                others = [\n",
    "                    f for f in detections[time_segment] if f != reference\n",
    "                ]  # All the other ARUs\n",
    "                others_in_distance = [\n",
    "                    aru for aru in others if aru in recorders_in_distance[reference]\n",
    "                ]  # ARUs close enough\n",
    "\n",
    "                if (\n",
    "                    len(others_in_distance) + 1 >= min_number_of_ARUs\n",
    "                ):  # minimum number of ARUs needed to localize.\n",
    "                    times.append(time_segment)\n",
    "                    reference_files.append(reference)\n",
    "                    other_files.append(others_in_distance)\n",
    "\n",
    "        grouped_detections = pd.DataFrame(\n",
    "            data=zip(times, reference_files, other_files),\n",
    "            columns=[\"time\", \"reference_file\", \"other_files\"],\n",
    "        )\n",
    "        return grouped_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Localizer(model= None, files = None, thresholds = None, predictions = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with a model\n",
    "from opensoundscape.torch.models.cnn import CNN\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "specky_table = pd.read_csv(Path(\"woodcock_labeled_data/woodcock_labels.csv\"))\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create a new dataframe with the filenames from the previous table as the index\n",
    "labels = pd.DataFrame(index=specky_table['filename'])\n",
    "\n",
    "#convert 'present' to 1 and 'absent' to 0\n",
    "labels['woodcock']=[1 if l=='present' else 0 for l in specky_table['woodcock']]\n",
    "\n",
    "#look at the first rows\n",
    "labels.head(3)\n",
    "train_df,validation_df = train_test_split(labels,test_size=0.2,random_state=1)\n",
    "classes = train_df.columns #in this case, there's just one class: [\"woodcock\"]\n",
    "\n",
    "model = CNN('resnet18',classes=classes,sample_duration=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['./woodcock_labeled_data/' + i for i in labels.index]\n",
    "a = Localizer(model = model, files = files, thresholds = {\"woodcock\":2}, predictions = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserWarning",
     "evalue": "Predictions already exist - set predictions to None if you want to re-run predictions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUserWarning\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_s/zm3gz4x52glfbflc25_3rp3w0000gq/T/ipykernel_99319/1057992104.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/_s/zm3gz4x52glfbflc25_3rp3w0000gq/T/ipykernel_99319/2885996663.py\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictions already exist - set predictions to None if you want to re-run predictions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUserWarning\u001b[0m: Predictions already exist - set predictions to None if you want to re-run predictions"
     ]
    }
   ],
   "source": [
    "a.get_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>woodcock</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">./woodcock_labeled_data/d4c40b6066b489518f8da83af1ee4984.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.5</th>\n",
       "      <td>-0.363851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <th>1.0</th>\n",
       "      <td>0.333593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>1.5</th>\n",
       "      <td>-0.242034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <td>-0.339171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/e84a4b60a4f2d049d73162ee99a7ead8.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.5</th>\n",
       "      <td>-0.399152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>./woodcock_labeled_data/78654b6f687d7635f50fba3546c7bdfa.wav</th>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <td>-0.407226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">./woodcock_labeled_data/ec0bd96aee95f03b47628b9c353eda49.wav</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.5</th>\n",
       "      <td>0.599427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <th>1.0</th>\n",
       "      <td>-0.290913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <th>1.5</th>\n",
       "      <td>-0.384412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <th>2.0</th>\n",
       "      <td>-0.289816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        woodcock\n",
       "file                                               start_time end_time          \n",
       "./woodcock_labeled_data/d4c40b6066b489518f8da83... 0.0        0.5      -0.363851\n",
       "                                                   0.5        1.0       0.333593\n",
       "                                                   1.0        1.5      -0.242034\n",
       "                                                   1.5        2.0      -0.339171\n",
       "./woodcock_labeled_data/e84a4b60a4f2d049d73162e... 0.0        0.5      -0.399152\n",
       "...                                                                          ...\n",
       "./woodcock_labeled_data/78654b6f687d7635f50fba3... 1.5        2.0      -0.407226\n",
       "./woodcock_labeled_data/ec0bd96aee95f03b47628b9... 0.0        0.5       0.599427\n",
       "                                                   0.5        1.0      -0.290913\n",
       "                                                   1.0        1.5      -0.384412\n",
       "                                                   1.5        2.0      -0.289816\n",
       "\n",
       "[116 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "None of the samples in the SafeDataset loaded. All samples caused exceptions during preprocessing. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_s/zm3gz4x52glfbflc25_3rp3w0000gq/T/ipykernel_99319/3611916600.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dev/opensoundscape/opensoundscape/torch/models/cnn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, samples, batch_size, num_workers, activation_layer, split_files_into_clips, overlap_fraction, final_clip, bypass_augmentations, invalid_samples_log, raise_errors, wandb_session, return_invalid_samples)\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0;31m# get batch of Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0mbatch_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/poetry_py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/poetry_py39/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/poetry_py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/poetry_py39/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/opensoundscape/opensoundscape/torch/safe_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mattempts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# loop around end to beginning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             raise IndexError(\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;34m\"None of the samples in the SafeDataset loaded. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;34m\"All samples caused exceptions during preprocessing. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: None of the samples in the SafeDataset loaded. All samples caused exceptions during preprocessing. "
     ]
    }
   ],
   "source": [
    "model.predict(files, activation_layer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d4c40b6066b489518f8da83af1ee4984.wav',\n",
       " 'e84a4b60a4f2d049d73162ee99a7ead8.wav',\n",
       " '79678c979ebb880d5ed6d56f26ba69ff.wav',\n",
       " '49890077267b569e142440fa39b3041c.wav',\n",
       " '0c453a87185d8c7ce05c5c5ac5d525dc.wav',\n",
       " '0fc107ec5e76bf7a98dd207a379ddeb6.wav',\n",
       " '50b6b7c7e843597e0dbc69869d10f4e8.wav',\n",
       " '35ca80c22127c3c0ae032a08adf8bf11.wav',\n",
       " 'f87d427bef752f5accbd8990f23c9499.wav',\n",
       " '0ab7732b506105717708ea95fb98de63.wav',\n",
       " 'ad90eefb6196ca83f9cf43b6f56c4b4a.wav',\n",
       " 'cd0b8d8a89321046e96abee21a5527ca.wav',\n",
       " '24073ce519bf1d24107da8a9e126b324.wav',\n",
       " '863095c237c52ec51cff7395d70cee41.wav',\n",
       " '882de25226ed989b31274eead6630b47.wav',\n",
       " '6a83b011665c482c1f260d8e111aa81c.wav',\n",
       " '45c4b1ed3d7d0acc27125579c5fcaf5c.wav',\n",
       " '4bb7dbc13db479e8b5769dd96be349f4.wav',\n",
       " '75b2f63e032dbd6d197900495a16856f.wav',\n",
       " '4afa902e823095e03ba23ebc398c35b7.wav',\n",
       " '01c5d0c90bd4652f308fd9c73feb1bf5.wav',\n",
       " '92647ab903049a9ee4125abdf7b24f2a.wav',\n",
       " 'c057a4486b25cd638850fc07399385b2.wav',\n",
       " 'e9e7153d11de3ac8fc3f7164d43bac92.wav',\n",
       " '724d8e61b678a6a897b47ed67d83ab7b.wav',\n",
       " 'ad14ac7ffa729060712b442e55aebf0b.wav',\n",
       " '0d043e9954d9d80ca2c3e86055e94487.wav',\n",
       " '78654b6f687d7635f50fba3546c7bdfa.wav',\n",
       " 'ec0bd96aee95f03b47628b9c353eda49.wav']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/d4c40b6066b489518f8da83af1ee4984.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/e84a4b60a4f2d049d73162ee99a7ead8.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/79678c979ebb880d5ed6d56f26ba69ff.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/49890077267b569e142440fa39b3041c.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/0c453a87185d8c7ce05c5c5ac5d525dc.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/0fc107ec5e76bf7a98dd207a379ddeb6.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/50b6b7c7e843597e0dbc69869d10f4e8.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/35ca80c22127c3c0ae032a08adf8bf11.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/f87d427bef752f5accbd8990f23c9499.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/0ab7732b506105717708ea95fb98de63.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/ad90eefb6196ca83f9cf43b6f56c4b4a.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/cd0b8d8a89321046e96abee21a5527ca.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/24073ce519bf1d24107da8a9e126b324.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/863095c237c52ec51cff7395d70cee41.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/882de25226ed989b31274eead6630b47.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/6a83b011665c482c1f260d8e111aa81c.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/45c4b1ed3d7d0acc27125579c5fcaf5c.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/4bb7dbc13db479e8b5769dd96be349f4.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/75b2f63e032dbd6d197900495a16856f.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/4afa902e823095e03ba23ebc398c35b7.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/01c5d0c90bd4652f308fd9c73feb1bf5.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/92647ab903049a9ee4125abdf7b24f2a.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/c057a4486b25cd638850fc07399385b2.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/e9e7153d11de3ac8fc3f7164d43bac92.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/724d8e61b678a6a897b47ed67d83ab7b.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/ad14ac7ffa729060712b442e55aebf0b.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/0d043e9954d9d80ca2c3e86055e94487.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/78654b6f687d7635f50fba3546c7bdfa.wav'),\n",
       " PosixPath('/Users/LOF19/Dev/opensoundscape/docs/tutorials/ec0bd96aee95f03b47628b9c353eda49.wav')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Path().resolve() / i for i in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ab727aedfeba4fe73c824599cc434fa16daf5621471b3748990173d78fc895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
